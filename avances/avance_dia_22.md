
# üìò D√≠a 22: Perceptr√≥n y Funciones de Activaci√≥n

## üß† ¬øQu√© es un Perceptr√≥n?
El **perceptr√≥n** es el bloque b√°sico de las redes neuronales artificiales. Fue propuesto por **Frank Rosenblatt en 1958** y es considerado el precursor del aprendizaje profundo.

Se basa en una idea simple:
- Recibe varias **entradas**.
- Cada entrada tiene un **peso** asignado.
- Calcula una **suma ponderada** de las entradas.
- Pasa ese valor por una **funci√≥n de activaci√≥n**.
- Genera una **salida binaria** (0 o 1).

## ‚öôÔ∏è Arquitectura del Perceptr√≥n

```
x1 ----\
        \        
x2 -----> (‚àë x*w + b) ---> funci√≥n de activaci√≥n ---> salida (0 o 1)
        /
x3 ----/
```

- **x1, x2, x3**: Entradas.
- **w**: Pesos.
- **b**: Sesgo (bias).
- **Funci√≥n de activaci√≥n**: Decide si la neurona "se activa" o no.

## üß™ F√≥rmula matem√°tica

```
y = f(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b)
```

- Donde `f` es la funci√≥n de activaci√≥n (por ejemplo: escal√≥n, sigmoide, ReLU).
- `y` es la salida del perceptr√≥n.

## üî• Funciones de Activaci√≥n

Las funciones de activaci√≥n permiten a las redes neuronales **introducir no linealidad**, algo crucial para resolver problemas complejos.

### 1. Funci√≥n Escal√≥n (Step Function)
```python
f(x) = 1 si x > 0, sino 0
```
- Uso original en perceptrones.
- No apta para redes profundas (no diferenciable).

### 2. Sigmoide
```python
f(x) = 1 / (1 + e^(-x))
```
- Rango: (0, 1).
- √ötil para problemas de clasificaci√≥n binaria.
- Problema: puede saturarse (gradientes peque√±os).

### 3. Tanh (Tangente hiperb√≥lica)
```python
f(x) = tanh(x)
```
- Rango: (-1, 1).
- Mejor que la sigmoide en redes profundas.

### 4. ReLU (Rectified Linear Unit)
```python
f(x) = max(0, x)
```
- Muy popular en redes neuronales profundas.
- R√°pido y eficiente.
- Problema: algunas neuronas pueden ‚Äúmorir‚Äù (si siempre reciben valores negativos).

### 5. Softmax
- Generaliza la sigmoide a m√∫ltiples clases.
- Se usa en la **√∫ltima capa** de una red para clasificaci√≥n multiclase.

## üìä ¬øPor qu√© son importantes estas funciones?
- **Linealidad ‚â† Aprendizaje complejo**
- Sin activaciones no lineales, la red neuronal es solo una regresi√≥n lineal.
- Las funciones de activaci√≥n permiten que la red **represente relaciones no lineales**.

## üß™ Ejemplo en Python (usando ReLU)

```python
import numpy as np  # Importamos la librer√≠a NumPy para operaciones num√©ricas

# ---------------------------
# Definici√≥n de la entrada de la neurona
# ---------------------------
X = np.array([1.2, -0.7, 3.0])         # Vector de caracter√≠sticas (entradas) de la neurona
pesos = np.array([0.5, -1.2, 0.8])     # Pesos sin√°pticos correspondientes a cada entrada
sesgo = 0.1                            # T√©rmino de sesgo (bias), desplazamiento del umbral

# ---------------------------
# C√°lculo de la suma ponderada
# ---------------------------
# Se calcula el producto punto entre las entradas y los pesos, y se le suma el sesgo
z = np.dot(X, pesos) + sesgo
print(f"Suma ponderada (z): {z}")     # Mostramos el resultado intermedio

# ---------------------------
# Definici√≥n de la funci√≥n de activaci√≥n ReLU
# ---------------------------
def relu(x):
    """
    Funci√≥n de activaci√≥n ReLU (Rectified Linear Unit).
    Si x es positivo, devuelve x; si es negativo, devuelve 0.
    """
    return max(0, x)

# ---------------------------
# Aplicamos ReLU a la suma ponderada
# ---------------------------
salida = relu(z)
print(f"Salida de la neurona: {salida}")  # Mostramos el resultado final tras la activaci√≥n

```

## üìö Recursos recomendados:
- üé• [Explicaci√≥n del Perceptr√≥n - StatQuest (YouTube)](https://www.youtube.com/watch?v=ntKn5TPHHAk)
- üìò [Libro: Neural Networks and Deep Learning - Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap1.html)